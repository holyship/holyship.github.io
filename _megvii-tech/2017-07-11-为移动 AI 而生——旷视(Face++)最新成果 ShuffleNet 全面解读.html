---
title: 为移动 AI 而生——旷视(Face++)最新成果 ShuffleNet 全面解读
author: Megvii旷视科技
wechat_source: >-
  http://mp.weixin.qq.com/s?timestamp=1500385423&src=3&ver=1&signature=FkzcjJJ48va4gLuFLNnuhgF5pIueUtXKRCI5Dd07rzptF5GFuiUwlmzQb8DFlwq7c6t2gsk6Uy*F6VOpkx-MEgYLohslFvtOsIyQUfxXYemwbsYS7nVzeIb9K7SWpXz9SQC5DBluBOmThnEZqoR9CGhJ-zTXwWwUXeSdwQRju*8=
date: '2017-07-11 00:00:00 +0000'

---

{% raw  %}
<section data-role="outer" label="Powered by 135editor.com"><section class=""><p><img src="http://mmbiz.qpic.cn/mmbiz_gif/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgBSpqh6OVfIZIqEbu7OcoW18W9L3ib8XSlMaICjDaqmiaofYC3iaEgDOaw/0?wx_fmt=gif" style="font-size: medium; width: 770px !important; height: 114.73px !important; visibility: visible !important;"></p></section><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">在视觉人工智能系统中，卷积神经网络（CNN）起着至关重要的作用。日前，旷视(Face++)研究院发表了《ShuffleNet：一种极高效的移动端卷积神经网络》一文，作者针对移动端低功耗设备提出了一种更为高效的卷积模型结构，在大幅降低模型计算复杂度的同时仍然保持了较高的识别精度，并在多个性能指标上均显著超过了同类方法。本文将对该成果进行详细解读。（论文下载地址：<span style="color: rgb(75, 172, 198);">https://arxiv.org/abs/1707.01083</span>）</span></p><p style="text-align: justify; text-indent: 0em;"><br></p><p style="text-align: justify; text-indent: 0em;"><br></p><section class="" data-tools="135编辑器" data-id="89887" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section class="" data-tools="135编辑器" data-id="89129" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="width:100%; text-align:center;" data-width="100%"><section data-role="width" style="display:inline-block;width:15px;vertical-align: top;"><img src="http://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgwtpoCPrfJJbSu7Ny3DMIemgyR9Umj7RicicJyicvrltnSY8CMAUz0OVvQ/0?wx_fmt=png" style="vertical-align: top; width: 15px !important; height: auto !important; visibility: visible !important;"></section><section style="display:inline-block;margin-left: 10px;color:#707070;font-size:18px;line-height: 36px;vertical-align: middle;"><strong class="" data-brushtype="text">设 计 思 想</strong></section></section></section></section><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">卷积神经网络是现代视觉人工智能系统的核心组件。近年来关于卷积模型的研究层出不穷，产生了如 VGG、ResNet、Xception 和 ResNeXt 等性能优异的网络结构，在多个视觉任务上超过了人类水平。然而，这些成功的模型往往伴随着巨大的计算复杂度（数十亿次浮点操作，甚至更多）。这就限制了此类模型只能用于高性能的服务器集群，而对于很多移动端应用（通常最多容许数百万至数千万次浮点操作）则无能为力。</span></p><p style="text-align: justify;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">解决这一难题的方法之一是设计更为轻量级的模型结构。现代卷积神经网络的绝大多数计算量集中在卷积操作上，因此高效的卷积层设计是减少网络复杂度的关键。其中，稀疏连接（sparse connection）是提高卷积运算效率的有效途径，当前不少优秀的卷积模型均沿用了这一思路。例如，谷歌的”Xception“网络<strong><span style="font-size: 12px;"><sup><span style="color: #FFC000;">[1]</span></sup></span></strong>引入了”深度可分离卷积”的概念，将普通的卷积运算拆分成逐通道卷积（depthwise convolution）和逐点卷积（pointwise convolution）两部进行，有效地减少了计算量和参数量；而 Facebook 的“ResNeXt”网络<strong><span style="color: rgb(255, 192, 0);"><sup>[2]</sup></span></strong>则首先使用逐点卷积减少输入特征的通道数，再利用计算量较小的分组卷积（group convolution）结构取代原有的卷积运算，同样可以减少整体的计算复杂度。</span></p><p style="text-align: justify;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">ShuffleNet 网络结构同样沿袭了稀疏连接的设计理念。作者通过分析 Xception 和 ResNeXt 模型，发现这两种结构通过卷积核拆分虽然计算复杂度均较原始卷积运算有所下降，然而拆分所产生的逐点卷积计算量却相当可观，成为了新的瓶颈。例如对于 ResNeXt 模型逐点卷积占据了 93.4% 的运算复杂度。可见，为了进一步提升模型的速度，就必须寻求更为高效的结构来取代逐点卷积。</span></p><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">受 ResNeXt 的启发，作者提出使用分组逐点卷积（group pointwise convolution）来代替原来的结构。通过将卷积运算的输入限制在每个组内，模型的计算量取得了显著的下降。然而这样做也带来了明显的问题：在多层逐点卷积堆叠时，模型的信息流被分割在各个组内，组与组之间没有信息交换</span><span style="font-size: 14px; color: #595959;">（如图 1(a) 所示）</span><span style="font-size: 14px; color: #595959;">。这将可能影响到模型的表示能力和识别精度。</span></p><p><br></p><p img-box="img-box" style="text-align: center;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHg92rPMBsjSjgCm87IMhQYzTl4fqsedS1CuFN7jD90NuLcqeO7fsmZXA/0?wx_fmt=jpeg" style="border-width: 0px; border-style: initial; border-color: initial; display: block; margin-right: auto; margin-left: auto; width: 640px !important; height: 249px !important;"><span style="font-size: 13px; color: #7F7F7F;">图 1 逐点卷积与通道重排操作</span></p><p><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">因此，在使用分组逐点卷积的同时，需要引入组间信息交换的机制。也就是说，对于第二层卷积而言，每个卷积核需要同时接收各组的特征作为输入，如图 1(b) 所示。作者指出，</span><span style="font-size: 14px; color: rgb(89, 89, 89);"><strong><span style="color: rgb(38, 38, 38);">通过引入“通道重排”（channel shuffle，见图 1(c) ）可以很方便地实现这一机制；并且由于通道重排操作是可导的，因此可以嵌在网络结构中实现端到端的学习</span></strong></span><span style="font-size: 14px; color: #595959;">。</span></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;"><br></span></p><h1 style="text-align: justify; font-weight: bold; line-height: 1.6em;"><span style="font-size: 14px; color: #595959;"></span></h1><p><br></p><section class="" data-tools="135编辑器" data-id="89887" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section class="" data-tools="135编辑器" data-id="89129" style="border-width: 0px; border-style: none; border-color: initial; box-sizing: border-box;"><section style="width:100%; text-align:center;" data-width="100%"><section data-role="width" style="display:inline-block;width:15px;vertical-align: top;"><img src="http://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgwtpoCPrfJJbSu7Ny3DMIemgyR9Umj7RicicJyicvrltnSY8CMAUz0OVvQ/0?wx_fmt=png" style="vertical-align: top; width: 15px !important; height: 36.9403px !important;"></section><section style="display:inline-block;margin-left: 10px;color:#707070;font-size:18px;line-height: 36px;vertical-align: middle;"><strong class="" data-brushtype="text">网 络 结 构</strong></section></section></section></section><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;"><br></span></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">基于分组逐点卷积和通道重排操作，作者提出了全新的 ShuffleNet 结构单元，如图 2 所示。该结构继承了“残差网络”（ResNet）<strong><sup><span style="color: rgb(255, 192, 0);">[3]</span></sup></strong>的设计思想，在此基础上做出了一系列改进来提升模型的效率：首先，使用逐通道卷积替换原有的 3x3 卷积，降低卷积操作抽取空间特征的复杂度，如图 2(a)所示；接着，将原先结构中前后两个 1x1 逐点卷积分组化，并在两层之间添加通道重排操作，进一步降低卷积运算的跨通道计算量。最终的结构单元如图 2(b) 所示。类似地，文中还提出了另一种结构单元（图2(c)），专门用于特征图的降采样。</span></p><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">借助 ShuffleNet 结构单元，作者构建了完整的 ShuffeNet 网络模型。<strong><span style="color: rgb(0, 0, 0);">它主要由 16 个 ShuffleNet 结构单元堆叠而成，分属网络的三个阶段，每经过一个阶段特征图的空间尺寸减半，而通道数翻倍。整个模型的总计算量约为 140 MFLOPs。</span></strong>通过简单地将各层通道数进行放缩，可以得到其他任意复杂度的模型。</span></p><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">另外可以发现，当卷积运算的分组数越多，模型的计算量就越低；这就意味着当总计算量一定时，较大的分组数可以允许较多的通道数，作者认为这将有利于网络编码更多的信息，提升模型的识别能力。</span></p><p><br></p><p img-box="img-box" style="text-align: center;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgW4Tdu4IoMrlh3wY89FdMSJ3qsAyyBfSLGyCgQGiaEaZuf3LdNzuLaCg/0?wx_fmt=jpeg" style="border-width: 0px; border-style: initial; border-color: initial; display: block; margin-right: auto; margin-left: auto; width: 640px !important; height: 354px !important;"><span style="font-size: 13px; color: #7F7F7F;">图 2 ShuffleNet 结构单元</span></p><p style="text-align: center;"><br></p><p style="text-align: center;"><br></p><section class="" data-tools="135编辑器" data-id="89887" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section class="" data-tools="135编辑器" data-id="89129" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="width:100%; text-align:center;" data-width="100%"><section data-role="width" style="display:inline-block;width:15px;vertical-align: top;"><img src="http://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgwtpoCPrfJJbSu7Ny3DMIemgyR9Umj7RicicJyicvrltnSY8CMAUz0OVvQ/0?wx_fmt=png" style="vertical-align: top; width: 15px !important; height: 36.9403px !important;"></section><section style="display:inline-block;margin-left: 10px;color:#707070;font-size:18px;line-height: 36px;vertical-align: middle;"><strong class="" data-brushtype="text">实 验 结 果</strong></section></section></section></section><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;"><br></span></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">作者通过一系列在 ImageNet 2016 分类数据集上的控制实验说明了 ShuffleNet 结构单元每个部件存在的必要性、对于其他网络结构单元的优越性。接着作者通过在 MS COCO 目标检测上的结果说明模型的泛化能力。最后，作者给出了在 ARM 计算平台上 ShuffleNet 实际运行时的加速效果。</span></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;"><br></span></p><section class="" data-tools="135编辑器" data-id="85908" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; " data-color="rgb(8, 179, 225)" data-custom="rgb(8, 179, 225)"><section style="width: 8px; margin: 5px; border-radius: 5px; display: inline-block; height: 25px; float: left; color: rgb(255, 255, 255); background-color: rgb(8, 179, 225); box-sizing: border-box;"></section><section style="display: inline-block; padding: 5px; box-sizing: border-box;"><p><span style="font-size: 18px; color: #08B3E1;" class="" data-brushtype="text">分组化逐点卷积</span></p></section></section><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">作者对于计算复杂度为 140 MFLOPs 、 40 MFLOPs、13 MFLOPs的 ShuffleNet 模型，在控制模型复杂度的同时对比了分组化逐点卷积的组数在1~8时分别对于性能的影响。从 表1 中可以看出，带有分组的(g&gt;1)的网络的始终比不带分组(g=1)的网络的错误率低。作者观察到对于较小的网络(如 ShuffleNet 0.25x )，较大的分组会得到更好结果，认为更宽的通道对于小网络尤其重要。受这点启发，作者移除了网络第三阶段的两个结构单元，将节省下来的运算量用来增加网络宽度后，网络性能进一步提高。</span></p><p><br></p><p img-box="img-box" style="text-align: center;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgnJBbckFvMdJCm5TCRo9YdUpYxPSvCaCF0s95j2PwRHaWuc3bLzeLLA/0?wx_fmt=jpeg" style="border-width: 0px; border-style: initial; border-color: initial; display: block; margin-right: auto; margin-left: auto; width: 770px !important; height: 240.625px !important;"><span style="font-size: 13px; color: #7F7F7F;">表1 组数对分类错误率的影响</span></p><p><br></p><section class="" data-tools="135编辑器" data-id="85908" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="width: 8px; margin: 5px; border-radius: 5px; display: inline-block; height: 25px; float: left; color: rgb(255, 255, 255); background-color: rgb(8, 179, 225); box-sizing: border-box;"></section><section style="display: inline-block; padding: 5px; box-sizing: border-box;"><p><span style="font-size: 18px; color: #08B3E1;" class="" data-brushtype="text">通道重排</span></p></section></section><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="color: #595959; font-size: 14px; text-indent: 2em;">通道重排的目的是使得组间信息能够互相交流。在实验中，有通道重排的网络始终优于没有通道重排的网络，错误率降低 0.9%~4.0%。尤其是在组数较大时(如g=8)，前者远远优于后者。</span></p><p style="text-align: justify;"><br></p><section class="" data-tools="135编辑器" data-id="85908" style="border-width: 0px; border-style: none; border-color: initial; box-sizing: border-box;"><section style="width: 8px; margin: 5px; border-radius: 5px; display: inline-block; height: 25px; float: left; color: rgb(255, 255, 255); background-color: rgb(8, 179, 225); box-sizing: border-box;"></section><section style="display: inline-block; padding: 5px; box-sizing: border-box;"><p><span style="font-size: 18px; color: #08B3E1;" class="" data-brushtype="text">对比其他结构单元</span></p></section></section><p style="text-align: justify;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">作者使用一样的整体网络布局，在保持计算复杂度的同时将 ShuffleNet 结构单元分别替换为 VGG-like、ResNet、Xception-like 和 ResNeXt 中的结构单元，使用完全一样训练方法。</span></p><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">表2 中的结果显示在不同的计算复杂度下，ShuffleNet 始终大大优于其他网络。</span></p><p><br></p><p img-box="img-box" style="text-align: center;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgIPicqax6wDjicUmz8q5w22v0iar9ibsPicKWD1T489me5fpOs8jl5flhyrA/0?wx_fmt=jpeg" style="border-width: 0px; border-style: initial; border-color: initial; display: block; margin-right: auto; margin-left: auto; width: 770px !important; height: 191.292px !important;"><span style="font-size: 13px; color: #7F7F7F;">表2 和其他网络结构的分类错误率对比（百分制）</span></p><p><br></p><section class="" data-tools="135编辑器" data-id="85908" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="width: 8px; margin: 5px; border-radius: 5px; display: inline-block; height: 25px; float: left; color: rgb(255, 255, 255); background-color: rgb(8, 179, 225); box-sizing: border-box;"></section><section style="display: inline-block; padding: 5px; box-sizing: border-box;"><p><span style="font-size: 18px; color: #08B3E1;" class="" data-brushtype="text">对比MobileNets</span><span style="color: #08B3E1; font-size: 12px;" class="" data-brushtype="text"><sub>和其他的一些网络结构</sub></span></p></section></section><p><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">最近 Howard et al. 提出了 MobileNets<sup><span style="color: rgb(255, 192, 0);"> <strong>[4]</strong></span></sup>，利用<strong><span style="font-size: 12px;"><sup><span style="color: #FFC000;">[1]</span></sup></span></strong>里的逐通道卷积的设计移动设备上高效的网络结构。<strong><span style="color: rgb(0, 0, 0);">虽然 ShuffleNet 是为了小于 150 MFLOPs 的模型设计的，在增大到 MobileNet 的 500~600 MFLOPs 量级，依然优于 MobileNet。而在 40 MFLOPs 量级，ShuffleNet 比 MobileNet 错误率低 6.7%。</span></strong>详细结果可以从表3中得到。</span></p><p><br></p><p img-box="img-box" style="text-align: center;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHg20HxdOYd3eGYb8UbRb0AOia4RZCYCoJ7VZWezicF3ZvbOjMNR41MNib1A/0?wx_fmt=jpeg" style="border-width: 0px; border-style: initial; border-color: initial; display: block; margin-right: auto; margin-left: auto; width: 640px !important; height: 261px !important;"><span style="font-size: 13px; color: #7F7F7F;">表3 ShuffleNet 和 MobileNet 对比</span></p><p><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">和其他一些网络结构相比，ShuffleNet 也体现出很大的优势。从表4中可以看出，ShuffleNet 0.5x 仅用 40 MFLOPs 就达到了 AlexNet 的性能，而 AlexNet 的计算复杂度达到了 720 MFLOPs，是 ShuffleNet 的 18 倍。<br></span></p><p><br></p><p img-box="img-box" style="text-align: center;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgZOL2T4pjvULU8icf3OcW4m9TA5FqrMrsbadkvlmdsCpAzIc91Viaic6nA/0?wx_fmt=jpeg" style="border-width: 0px; border-style: initial; border-color: initial; display: block; margin-right: auto; margin-left: auto; width: 640px !important; height: 245px !important;"><span style="font-size: 13px; color: #7F7F7F;">表4 ShuffleNet 和其他网络结构计算复杂度的对比</span></p><p style="text-align: justify;"><br></p><section class="" data-tools="135编辑器" data-id="85908" style="border-width: 0px; border-style: none; border-color: initial; box-sizing: border-box;"><section style="width: 8px; margin: 5px; border-radius: 5px; display: inline-block; height: 25px; float: left; color: rgb(255, 255, 255); background-color: rgb(8, 179, 225); box-sizing: border-box;"></section><section style="display: inline-block; padding: 5px; box-sizing: border-box;"><p><span style="font-size: 18px; color: #08B3E1;" class="" data-brushtype="text">MS COCO物体检测</span></p></section></section><p style="text-align: justify;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">在 Faster-RCNN <strong><sup><span style="color: rgb(255, 192, 0);">[5]</span></sup></strong><sup><span style="color: rgb(255, 192, 0);"> </span></sup>框架下，和 1.0 MobileNet-224 网络复杂度可比的 ShuffleNet 2x，在 600 分辨率的图上的 mAP 达到 24.5%，而 MobileNet 为 19.8%，表明网络在检测任务上良好的泛化能力。</span></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;"><br></span></p><section class="" data-tools="135编辑器" data-id="85908" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="width: 8px; margin: 5px; border-radius: 5px; display: inline-block; height: 25px; float: left; color: rgb(255, 255, 255); background-color: rgb(8, 179, 225); box-sizing: border-box;"></section><section style="display: inline-block; padding: 5px; box-sizing: border-box;"><p><span style="font-size: 18px; color: #08B3E1;" class="" data-brushtype="text">实际运行速度</span></p></section></section><p style="text-align: justify;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">最后作者在一款 ARM 平台上测试了网络的实际运行速度。在作者的实现里 40 MFLOPs 的 ShuffleNet 对比相似精度的 AlexNet 实际运行速度快约 13x 倍。224 x 224 输入下只需 15.2 毫秒便可完成一次推理，在 1280 x 720 的输入下也只需要 260.1 毫秒。</span></p><h1 style="text-align: justify; font-weight: bold; line-height: 1.6em;"><span style="font-size: 14px; color: #595959;"></span></h1><p><br></p><p><br></p><section class="" data-tools="135编辑器" data-id="89129" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="width:100%; text-align:center;" data-width="100%"><section data-role="width" style="display:inline-block;width:15px;vertical-align: top;"><img src="http://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgwtpoCPrfJJbSu7Ny3DMIemgyR9Umj7RicicJyicvrltnSY8CMAUz0OVvQ/0?wx_fmt=png" style="vertical-align: top; width: 15px !important; height: 36.9403px !important;"></section><section style="display:inline-block;margin-left: 10px;color:#707070;font-size:18px;line-height: 36px;vertical-align: middle;"><strong class="" data-brushtype="text">应 用 展 望</strong></section></section></section><p style="text-align: justify; text-indent: 2em;"><br></p><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;">很多重要的需要语义信息的计算机视觉任务，如目标检测、物体识别等，都需要一个好的“基础模型”作为特征提取器。在移动设备越发重要的今天，在其之上运行的视觉算法模型会越多、准确率要求会越来越高。</span></p><p><br></p><section data-role="outer" label="Powered by 135editor.com" style="font-family:微软雅黑;font-size:16px;"><section class="" data-tools="135编辑器" data-id="85739" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="margin-right: auto; margin-left: auto;" class=""><section style="text-align: center; border-width: 0px; border-style: none; border-color: initial; margin-bottom: 70px; box-sizing: border-box;"><section style="box-sizing: border-box;text-align: left; "><section style="width:150px;height:150px;display:inline-block;"><section data-role="circle" style="border-radius: 100%; overflow: hidden; margin-right: auto; margin-left: auto; width: 100%; padding-bottom: 100%; height: 0px; background-image: url(&quot;http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgW7P9NDEibUEWMSduI1h62M4Fqun7t8XicOJVk7k9oAUoogZz8UcuobkQ/0?wx_fmt=jpeg&quot;); background-position: 50% 50%; background-size: cover; box-sizing: border-box;" data-width="100%"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgW7P9NDEibUEWMSduI1h62M4Fqun7t8XicOJVk7k9oAUoogZz8UcuobkQ/0?wx_fmt=jpeg" style="opacity: 0; width: 150px !important; height: 150px !important;"></section></section></section><section style="box-sizing: border-box; margin-top: -50px;"><section style="height:120px;width:120px;display:inline-block;"><section data-role="circle" style="border-radius: 100%; overflow: hidden; margin-right: auto; margin-left: auto; width: 100%; padding-bottom: 100%; height: 0px; background-image: url(&quot;http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHghV1mo6iaEVlfSb9U3y3ulibvm9ZhfcGzn8hsum7ibZbqniad0Iic4LXdRiag/0?wx_fmt=jpeg&quot;); background-position: 50% 50%; background-size: cover; box-sizing: border-box;" data-width="100%"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHghV1mo6iaEVlfSb9U3y3ulibvm9ZhfcGzn8hsum7ibZbqniad0Iic4LXdRiag/0?wx_fmt=jpeg" style="z-index: -1; cursor: pointer; white-space: pre-wrap; width: 120px !important; height: 120px !important;"></section></section></section><section style="box-sizing: border-box; padding-right: 0.1em; text-align: right; margin-top: -220px;"><section style="width:150px;height:150px;display:inline-block;"><section data-role="circle" style="border-radius: 100%; overflow: hidden; margin-right: auto; margin-left: auto; width: 100%; padding-bottom: 100%; height: 0px; background-image: url(&quot;http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHg2tyicOkzuj7ng3ZTqFpv5VUfhlN7X5UeNZWA5X0T7rezqMp9q3sQXGw/0?wx_fmt=jpeg&quot;); background-position: 50% 50%; background-size: cover; box-sizing: border-box;" data-width="100%"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHg2tyicOkzuj7ng3ZTqFpv5VUfhlN7X5UeNZWA5X0T7rezqMp9q3sQXGw/0?wx_fmt=jpeg" style="opacity: 0; width: 150px !important; height: 150px !important;"></section></section></section></section></section></section><p><br></p></section><p style="text-align: justify; text-indent: 2em;"><span style="font-size: 14px; color: #595959;"><span style="color: rgb(0, 0, 0);"><strong>无论目标检测和识别、人脸检测和识别，还是图片风格化编辑、美颜，抑或是直播用户行为分析，都离不开基础模型的支持。好的基础模型可以让因为原始运算力需求过大而无法在手机上高效运行的模型能高效运行，将众多不可能变为可能。</strong></span></span></p><p style="text-indent: 32px; text-align: justify;"><br></p><p style="text-indent: 32px; text-align: justify;"><span style="font-size: 14px; text-indent: 2em;">此外，其他常用的模型压缩技术，如稀疏化、网络量化等技术也可以在 ShuffleNet 上应用，提高存储效率和运行速度，进一步降低视觉算法和应用的落地门槛。</span></p><p style="text-align: justify; text-indent: 0em;"><br></p><section class="" data-tools="135编辑器" data-id="85891" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; " data-color="rgb(8, 179, 225)" data-custom="rgb(8, 179, 225)"><section style="text-align:center;"><section data-bgless="lighten" style="display: inline-block; color: rgb(255, 255, 255); background-color: rgb(135, 226, 251);"><section style="padding-top: 8px; padding-right: 8px; background-image: repeating-linear-gradient(-45deg, transparent, transparent 1px, rgb(254, 254, 254) 4px, rgb(254, 254, 254) 8px); box-sizing: border-box;"><section style="padding: 6px 20px; background-color: rgb(8, 179, 225); box-sizing: border-box;"><p><span class="" data-brushtype="text">参 考 文 献</span></p></section></section></section><section style="padding: 40px 20px 20px; margin-top: -20px; border-right: 1px solid rgb(182, 182, 182); border-bottom: 1px solid rgb(8, 179, 225); background-color: rgb(252, 252, 252); box-sizing: border-box;" class=""><p style="text-align: justify; margin-bottom: 10px;"><span style="font-size: 12px; color: #FFC000;"><em>[1] </em></span><span style="font-size: 12px;"><em><span style="color: #595959;">François Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv:1610.02357, 2016.</span></em></span></p><p style="text-align: justify; margin-bottom: 10px;"><span style="font-size: 12px; color: #FFC000;"><em>[2] </em></span><span style="font-size: 12px;"><em><span style="color: rgb(89, 89, 89);">Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. arXiv:1611.05431, 2016.</span></em></span></p><p style="text-align: justify; margin-bottom: 10px;"><span style="font-size: 12px; color: #FFC000;"><em>[3] </em></span><span style="font-size: 12px;"><em><span style="color: rgb(89, 89, 89);">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.</span></em></span></p><p style="text-align: justify; margin-bottom: 10px;"><span style="font-size: 12px; color: #FFC000;"><em>[4]</em></span><span style="font-size: 12px;"><em><span style="color: rgb(89, 89, 89);"> Howard, Andrew G., et al. "Mobilenets: Efficient convolutional neural networks for mobile vision applications." arXiv preprint arXiv:1704.04861 (2017).</span></em></span></p><p style="text-align: justify; margin-bottom: 10px;"><span style="font-size: 12px; color: #FFC000;"><em>[5]</em></span><span style="font-size: 12px;"><em><span style="color: rgb(89, 89, 89);"> Ren, Shaoqing, et al. "Faster R-CNN: Towards real-time object detection with region proposal networks." Advances in neural information processing systems. 2015.</span></em></span></p></section></section></section><section class="" data-tools="135编辑器" data-id="64737" style="border-width: 0px; border-style: none; border-color: initial; box-sizing: border-box;"><p style="text-align: center; white-space: normal;"><img src="http://mmbiz.qpic.cn/mmbiz/cZV2hRpuAPia3RFX6Mvw06kePJ7HbmI7b35o17yNJx4WHYPSQj280IElEicRPq2CviaJe8fjL2AeadmIjARqVZWnw/0.jpeg" style="display: inline; width: 662px !important; height: 2px !important;"></p></section><section data-role="paragraph" class="" style="border-width: 0px; border-style: none; border-color: initial; box-sizing: border-box;"><p style="white-space: normal;"><br></p></section><section class="" data-tools="135编辑器" data-id="127" data-color="#08B3E1" data-custom="#08B3E1" style=" border-width: 0px; border-style: none; border-color: initial;  box-sizing: border-box; "><section style="margin: 60px 16px 16px; border-width: 1px; border-style: solid; border-color: rgb(8, 179, 225); text-align: center; border-radius: 8px; font-size: 18px; font-weight: inherit; text-decoration: inherit; box-sizing: border-box;"><section style="margin-top: -3.3em; margin-right: 5px; margin-left: 5px; color: inherit;"><section style="border-width: 2px; border-style: solid; border-color: rgb(8, 179, 225); width: 108px; clear: both; margin-right: auto; margin-left: auto; height: 108px; border-radius: 50%; box-shadow: rgb(201, 201, 201) 0px 2px 2px 2px; box-sizing: border-box;"><img src="http://mmbiz.qpic.cn/mmbiz_gif/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHg26lERyPsB6MWTGQ0d2sFvibhqFAuTZXqJx63wPXibvjtPV9vOZC0HoqQ/0?wx_fmt=gif" style="border-radius: 50%; color: inherit; display: inline-block; width: 104px !important; height: 104px !important;"></section></section><section class="" data-brushtype="text" data-style="text-align: left; font-size: 14px; color: inherit;" style="margin: 8px 15px; line-height: 1.4; color: inherit;"><p style="white-space: normal;"><em><strong><span style="color: #595959;">Power &nbsp;Human &nbsp;with &nbsp;AI.</span></strong></em></p></section><section style="margin: 10px 15px; border-top: 1px solid rgb(8, 179, 225); border-right-color: rgb(8, 179, 225); border-bottom-color: rgb(8, 179, 225); border-left-color: rgb(8, 179, 225); color: inherit; box-sizing: border-box;"><p style="white-space: normal;"><br></p><p style="white-space: normal;"><strong><span style="font-size: 14px; color: #08B3E1;">长按二维码关注旷视(Face++)</span></strong></p><p style="white-space: normal;"><strong><span style="font-size: 14px; color: #08B3E1;">让 机 器 看 懂 世 界</span></strong></p><p style="white-space: normal;"><strong><span style="font-size: 14px; color: #08B3E1;">全球领先的图像识别服务</span></strong></p><p style="white-space: normal;"><strong><span style="font-size: 14px; color: #08B3E1;">www.megvii.com</span></strong></p><p style="white-space: normal;"><strong><span style="font-size: 14px; color: #08B3E1;">✆ 400-6700-866</span></strong></p></section><section data-role="width" style="display:inline-block;width:150px"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/MR4y3Zyg39ww79R9ib4VpSQtiaHv7D6fHgHD7cvXcxhhMUAxvf8UyerpicGENkHnMWcrnEpYn4MHZibsu6b199Gmicg/0?wx_fmt=jpeg" style="color: inherit; margin-bottom: 5.4px; visibility: visible !important; width: 150px !important; height: 150px !important;"></section></section></section></section><hr/><a href="http://mp.weixin.qq.com/s?timestamp=1500385423&src=3&ver=1&signature=FkzcjJJ48va4gLuFLNnuhgF5pIueUtXKRCI5Dd07rzptF5GFuiUwlmzQb8DFlwq7c6t2gsk6Uy*F6VOpkx-MEgYLohslFvtOsIyQUfxXYemwbsYS7nVzeIb9K7SWpXz9SQC5DBluBOmThnEZqoR9CGhJ-zTXwWwUXeSdwQRju*8=">微信地址</a>
{% endraw  %}

