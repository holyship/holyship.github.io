---
title: 实时移动通信中基于时空域联合约束的低照度视频增强技术研究
author: 微信多媒体团队
wechat_source: >-
  http://mp.weixin.qq.com/s?timestamp=1498052765&src=3&ver=1&signature=nULoNfhbsabgcFJEB9PUQ4-HiCbVGjE0gR-2o1d1M*MY48nqNmiBv2r6DzykYQl3fvTZc7EMIgnaeiluKD1fhLHqZy3hFPiyZ1pw9x6awF7WBQs5Mg0BH*-YZVYk-ivKO8QVWATss22Q23Ik0*7WIWNIECKhtZw7zASPQYxM7*M=
date: '2017-06-20 00:00:00 +0000'

---

{% raw  %}
<h2 class="" style="margin-bottom: 14px; padding-bottom: 10px; font-size: 24px; line-height: 1.4; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(231, 231, 235);"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/UqFrHRLeCAlgYS4WZsamDmXCj4Nozv1RRnfkVYT2icekMQ6Rgc8OSOhULAGWXu9VOL2Cac6nJggBMwkdcaLQplA/640?wx_fmt=jpeg" style="line-height: 1.6; box-sizing: border-box !important; word-wrap: break-word !important; visibility: visible !important; width: 640px !important; height: 360px !important;"></h2><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><span style="max-width: 100%; color: rgb(136, 136, 136); box-sizing: border-box !important; word-wrap: break-word !important;"><em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">视频通话是微信的基础功能之一，在实际应用中受光照条件及视频采集设备能力所限，视频发暗是影响主观体验的重要因素。我们尝试改进这个问题，欢迎留言交流:)</em></span></p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><span style="max-width: 100%; color: rgb(136, 136, 136); box-sizing: border-box !important; word-wrap: break-word !important;"><em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">该项工作的主要成果发表在ISCAS 2017国际会议上。（"Low-Lighting Video Enhancement Using Constrained Spatial-Temporal Model for Real-Time Mobile Communication", ISCAS, pp：595-598, Baltimore, MD, USA, 2017）</em></span></p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><span style="max-width: 100%; color: rgb(0, 128, 255); box-sizing: border-box !important; word-wrap: break-word !important;"><em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">http://iscas2017.org/</em></span></p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">应用背景</h2><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">目前绝大多数智能手机具有了视频拍摄功能，但由于受镜头尺寸和成本的限制，采集的视频图像的单像素上的光通量较小。尤其室内场景光照不足或者低照度的情况下，部分手机由于曝光不足导致视频明显偏暗，限制了其实时的移动视频通话的应用。低照度视频增强技术，是一种通过修改视频图像的像素值来有效的改善此类场景下的视频效果，提升客户的主观感受的视频图像处理技术。通过该技术来弥补低照度下手机拍摄的视频图像，可以扩大视频通话的应用场景，提升用户的产品体验。</p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">相关技术</h2><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">现有的低照度视频图像增强技术主要借鉴低照度图像增强的一些方法，具体举几个例子：例如，直方图均衡(Hist Equalization)：增强曲线为图像的概率累积分布函数，该方法能最大程度地拉伸整个图像的对比度。由于暗场景的图像其直方图存在高峰，经过直方图均衡处理后，导致图像过度增强，从而使得图像失真，同时容易放大噪声。例如，对比度拉伸(Contrast Stretching)：通过设计合适的映射曲线，控制全局各个灰阶增强幅度。但映射曲线不具备自适应性，对不同图像要设计专门的增强曲线，才可以达到增强图像的效果。Gamma校正是对比度拉伸的一种。例如，同态滤波(Homomorphic Filtering)：是一种频域增强算法。像素值由光照分量和反射分量决定。其中光照分量位于低频段，反射分量位于高频段。将图像映射到频域后，将光照分量和反射分量在频域分开，再分别进行增强处理。该方法适合光照不均匀情况下的增强，如同时包含室内和室外场景。因此不适应于视频通话中的增强。其他的增强算法，如色调映射(Tone Mapping)和Retinex等方法，存在计算量过大或者容易在边缘处产生光晕等缺点，都不能直接用于实时视频增强的场合。</p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">设计动机</h2><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">在实时的移动视频通话的应用条件下，我们简要说明现有技术的缺点及要解决的问题：</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">（1）现有的视频增强方案主要是借鉴单幅图像的增强方法，因此只考虑单帧信息，没有考虑相邻帧之间的相关性。导致各帧间增强幅度不一致，从而出现闪烁或者颗粒现象，降低了主观效果。且对于亮度正常的图像，其增强算法往往过渡处理。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">（2）在实时视频聊天的场景中，对计算量及存储空间非常敏感。实用算法都必须在较小的计算量和存储需求，达到最佳的增强效果。而类似色调映射和Retinex的计算量过大，即便经过优化也无法适应手机端的实时视频处理。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">通过分析已有技术的技术特点和应用条件的限制因素，我们希望设计的算法具有以下几个特点：其一，增强低照度的视频图像。其二，连续的视频图像不产生闪烁。其三，对亮度正常的图像不做夸张的再增强。其四，算法的计算复杂度能够满足实时移动视频通话的限制。这样的设计思路，为后续的算法设计和实验验证提供了方向和标准。</p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">时空域联合约束的低照度视频增强技术</h2><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">本文提出了一种实时移动通信中基于时空域联合约束的低照度视频增强技术。在该技术中我们设计了图像空域的亮度增强约束和对比度增强约束，以及视频帧时域的亮度一致性约束，并对提出的联合约束框架给出了凸优化的闭合解。下面详细对每一个约束的设计进行详尽介绍，最后给出问题的优化解。该方法在YUV420空间进行，将亮度分量Y和色度分量UV分开，只对Y分量进行处理，保持UV颜色信息。图像灰度的取值范围为[0-255]。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">首先，低照度视频图像增强的最重要的处理就是亮度增强。一种最为直接的方式就是使用一族增强函数来定义亮度值增强，如图1所示：</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj03oh400qLjjdkCgF9ypU4OB0pbgYxOId5pEDRgN6323VmE8gC7r1B8Q/0?" style="border: 1px solid rgb(238, 237, 235); background-image: none; background-color: rgb(238, 237, 235); background-size: 22px; background-position: 50% 50%; background-repeat: no-repeat; box-sizing: border-box !important; word-wrap: break-word !important; width: 442px !important; height: 340.462px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图1.&nbsp;亮度值的增强函数族（横轴为低照度亮度值，纵轴为增强后的亮度值）。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">在我们的设计中，通过采集同一场景下低照度和正常照度的视频数据对，通过离线训练的方式，得到了基于训练样本的亮度增强函数族F<sub style="max-width: 100%; font-size: 0.83em; box-sizing: border-box !important; word-wrap: break-word !important;">I</sub>。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;&nbsp;&nbsp;&nbsp;然而，由于单纯的提升整体像素的亮度值，会使得图像整体的对比度不均衡，仍然不能提供好的人眼主观视觉感受。我们提出了自适应的视频图像亮度值域调整的算法，进而通过新的值域范围来对图像进行直方图的均衡化调整。我们统计图像的像素点的值域范围时，排除掉最小的d%个像素和最大的d%个像素的干扰，将中间范围内的像素最大值和最小值进行调整，调整策略为自适应软阈值的方法。从而生成数据自适应的对比度增强函数F<sub style="max-width: 100%; font-size: 0.83em; box-sizing: border-box !important; word-wrap: break-word !important;">C</sub>。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;&nbsp;&nbsp;&nbsp;只从单一的图像维度来考虑增强的问题，往往会产生相邻图像帧之间的亮度跳变，即闪烁现象。为此我们将当前帧和相邻多帧的平均亮度的差异代价，构造出代价函数G，来约束由亮度增强和对比度增强函数可能带来的闪烁现象。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;&nbsp;&nbsp;&nbsp;综上我们设计的优化问题为：</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0e7WicUZOhzl8icVE1xudEOs2EyXEKJRhWBlW17S4WbibA4r9GNPpVMYAQ/640?wx_fmt=png" style="box-sizing: border-box !important; word-wrap: break-word !important; visibility: visible !important; width: 640px !important; height: 80px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">易知三个约束项均为二次项，问题可以通过最小二乘法求解，如下：</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0T92IMeP7qMsdoTulgvVHPHgJzM26SQPoUAN0fkjvWerej60DzTyV1A/640?wx_fmt=png" style="box-sizing: border-box !important; word-wrap: break-word !important; visibility: visible !important; width: 640px !important; height: 243px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">本文算法的创新性和贡献：</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">（1）通过离线的方法训练亮度增强函数，用以合理的提升亮度。根据当前低照度视频图像自适应的生成对比度增强函数，用以重新调整图像对比度。根据已处理帧的信息，自适应的调整临近帧间的亮度一致性，抑制闪烁现象。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">（2）有效的将上述（1）的影响因素，统一成正则化的最优化框架下，来同时约束满足上文前三点项设计要求（增强低照度的视频图像，连续的视频图像不产生闪烁以及对亮度正常的图像不做夸张的再增强），并给出满足实时应用需求的求解方式。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">（3）该算法具有较低的计算复杂度和极强的鲁棒性，实验证明其大量测试和线上的视频图像的增强效果中没有过度增强和失真增强的差质量样例。</p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">实验结果</h2><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">我们通过实验数据来验证我们的方法的性能。首先我们给出不同场景下不同暗图像增强算法的主观性能比较。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0Tsb5IztzN4HSA2wNofMOok5mWBR8wxPzoKBmPS8uOtQtZ4X4ticCwSw/640?wx_fmt=png" style="box-sizing: border-box !important; word-wrap: break-word !important; visibility: visible !important; width: 640px !important; height: 604px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图2. Bonsai图片上的主观实验结果，a表示原始暗视频图像，b-e分别表示文献[1][2][3][4]的方法，f表示我们提出的方法处理的视频图像。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0xmcFHYs124A7PazWbrIJSqhMEiciblEOQTzUo12mJlasEMibOc20Xbxcg/0?" style="border: 1px solid rgb(238, 237, 235); background-image: none; background-color: rgb(238, 237, 235); background-size: 22px; background-position: 50% 50%; background-repeat: no-repeat; box-sizing: border-box !important; word-wrap: break-word !important; width: 670px !important; height: 709.137px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图3. Face图片上的主观实验结果，a表示原始暗视频图像，b-e分别表示文献[1][2][3][4]的方法，f表示我们提出的方法处理的视频图像。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图2和图3分别给出了低照度条件下景物和人物的增强效果。从图中可以看出我们的方法具有相对较好的主观效果。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0moXn2M2aAzePUiawgLxibCRVpAcQpvNlNl0sB8kExgYuWLYpsSCkN03Q/0?" style="border: 1px solid rgb(238, 237, 235); background-image: none; background-color: rgb(238, 237, 235); background-size: 22px; background-position: 50% 50%; background-repeat: no-repeat; box-sizing: border-box !important; word-wrap: break-word !important; width: 670px !important; height: 889.096px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图4. Street视频上连续两帧的主观实验结果，a表示原始暗视频图像，b-e分别表示文献[1][2][3][4]的方法，f表示我们提出的方法处理的视频图像。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图4给出了连续的视频帧中抑制闪烁效果的主观结果。如图可以看出一些方法存在着视频图像亮度闪烁的现象，我们的方法在增强亮度的同时不存在闪烁的现象。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0NzwuERCFSWALibwOONicQHx4Jp5Hkep9wTwgg7ZDr21cN6JqGrlicbZzg/0?" style="border: 1px solid rgb(238, 237, 235); background-image: none; background-color: rgb(238, 237, 235); background-size: 22px; background-position: 50% 50%; background-repeat: no-repeat; box-sizing: border-box !important; word-wrap: break-word !important; width: 670px !important; height: 627.944px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图5. Kimino视频图像的主观实验结果，a表示原始暗视频图像，b-e分别表示文献[1][2][3][4]的方法，f表示我们提出的方法处理的视频图像。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">图5给出了常规亮度的视频帧，我们的方法能够尽可能的不影响已经足够明亮的视频图像，避免了过度的增强。</p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">进而我们通过一系列的客观的数据来从另一个侧面评估我们的算法。表1中，我们通过将算法统一到Matlab平台上进行公平的比较，分析各个算法的时间复杂度，可以看出我们的方法具有最好的实时性能，且在实际应用中该算法仅仅需要非常少的CPU计算代价，且具有良好的汇编优化效果，并不需要大规模的GPU等其他复杂计算资源。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAlgYS4WZsamDmXCj4Nozv1Rwbw6H1eZNia9JZcNzYibfz4d1JwGPhDHcMsHmgGUonWWYmeO91TEQlyA/0?" style="border: 1px solid rgb(238, 237, 235); background-image: none; background-color: rgb(238, 237, 235); background-size: 22px; background-position: 50% 50%; background-repeat: no-repeat; box-sizing: border-box !important; word-wrap: break-word !important; width: 670px !important; height: 379.604px !important;"></p><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">表2中，我们通过利用无参考的质量评价NR-CDIQA方法，分析各个算法的客观质量，可以看出我们的方法具有最好的视觉性能。值得注意的是，在常规亮度的视频图像Kimino中，我们方法处理的得分最接近于原视频图像的打分，即我们没有夸张的过度增强。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAlgYS4WZsamDmXCj4Nozv1RVg32cmUpAKmibEoky5GkT0z9moiaZEwT12ypRBasdT05Iq2icicezJDvqA/0?" style="border: 1px solid rgb(238, 237, 235); background-image: none; background-color: rgb(238, 237, 235); background-size: 22px; background-position: 50% 50%; background-repeat: no-repeat; box-sizing: border-box !important; word-wrap: break-word !important; width: 670px !important; height: 435.549px !important;"></p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">参考文献</h2><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;"><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"></span></h2><p class="" style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;"><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">[1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">K. He, J. Sun, and X. Tang, “Single Image Haze Removal Using Dark Channel Prior,” in&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Proc. IEEE Conf. Computer Vision and Pattern Recognition.</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">, pp. 1956-1963, 2009.</span></p><p class="" style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;"><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">[2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Y. Li, Robby T. Tan, and Michael S. Brown,&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">“</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Nighttime Haze Removal with Glow and Multiple Light Colors,</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">”</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">in&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Proc.</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;IEEE International Conference on Computer Vision, pp. 226-234, 2015.</span></p><p class="" style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;"><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">X. Dong, G. Wang, Y. Pang, W. Li, J. Wen, W. Meng and Lu, Y,&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">“</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Fast efficient algorithm for enhancement of low lighting video,</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">”</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;in&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Proc.</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;IEEE International Conference on Multimedia and Expo, 2011.</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"></span></p><p class="" style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;"><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">[4]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">W. Shi, C. Chen, F. Jiang, D. Zhao and W. Shen,&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">“</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Group-based sparse representation for low lighting image enhancement,</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">”</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">in&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Proc.&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">IEEE International Conference on Image Processing, pp. 4082-4086, 2016.</span></p><p class="" style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;"><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">[5]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">Y. Fang, K. Ma, Z. Wang, W. Lin, Z. Fang, and G. Zhai,&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">“</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">No-Reference Quality Assessment of Contrast-Distorted Images Based on Natural Scene Statistics,</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">”</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">&nbsp;IEEE Signal Processing Letter,&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">vol. 2</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">2</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">, no.&nbsp;</span><span lang="EN-US" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">7, pp. 838-842, 2015.</span></p><h2 style="margin-top: 1.02em; margin-bottom: 1.3em; font-weight: bolder; font-size: 24px; max-width: 100%; unicode-bidi: embed; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">结语</h2><p style="margin-bottom: 1.4em; max-width: 100%; min-height: 1.8em; unicode-bidi: embed; line-height: 29px; color: rgb(68, 68, 68); box-sizing: border-box !important; word-wrap: break-word !important;">本文针对低照度视频序列，为了增强视频图像的主观视觉质量，设计了正则化的最优化框架（其中包含：亮度增强代价函数，对比度增强代价函数和亮度一致性代价函数），并给出满足实时应用需求的求解方式。该算法具有较低的计算复杂度和极强的鲁棒性，实验证明其大量测试和线上的视频图像的增强效果中没有过度增强和失真增强的差质量样例。相关技术全部为组内自研，已获中国专利授权一项且中稿国际视频编码领域的重要会议文章一篇。</p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><img src="http://mmbiz.qpic.cn/mmbiz_jpg/UqFrHRLeCAkNHLFmTAZqtno1xZjI0sj0kTpCkM8QPlQJwuXVEjpgFmk9JOvNuqiaQ0aJwlmNw3ZPENyvFlR0SeQ/640?wx_fmt=jpeg" style="box-sizing: border-box !important; word-wrap: break-word !important; visibility: visible !important; width: 640px !important; height: 446px !important;"></p><p style="max-width: 100%; min-height: 1em; box-sizing: border-box !important; word-wrap: break-word !important;"><br></p><p>更多精彩内容，可扫描二维码关注微信后台团队公众号</p><p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/csvJ6rH9McsqWffXGmZuBCtygdm1KxU1csicuv24yk9DrRlK0jbGIAMibVxrytJFmzayT1Oya6OghD3ftviaAWbYQ/0?wx_fmt=jpeg" style="width: 430px !important; height: 430px !important;"></p><hr/><a href="http://mp.weixin.qq.com/s?timestamp=1498052765&src=3&ver=1&signature=nULoNfhbsabgcFJEB9PUQ4-HiCbVGjE0gR-2o1d1M*MY48nqNmiBv2r6DzykYQl3fvTZc7EMIgnaeiluKD1fhLHqZy3hFPiyZ1pw9x6awF7WBQs5Mg0BH*-YZVYk-ivKO8QVWATss22Q23Ik0*7WIWNIECKhtZw7zASPQYxM7*M=">微信地址</a> | <a href="https://mp.weixin.qq.com/s/0N83XatQxAfw90VimvU0ow#rd">阅读原文</a>
{% endraw  %}

